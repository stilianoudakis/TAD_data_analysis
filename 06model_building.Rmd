---
title: "Model Building"
author: "Spiro Stilianoudakis"
date: "May 13, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loading Libraries

```{r}
#library(MultiAssayExperiment)
library(GenomicRanges)
#library(IRanges)
library(caret)
library(data.table)
library(gbm)
library(pROC)
library(plyr)
library(dplyr)
library(DMwR)
library(pROC)
```

# Reading in data

```{r}
setwd("C:/Users/Spiro Stilianoudakis/Documents/TAD_data/data")

logitdata <- readRDS("logitdata_2.rds")

```

#Transformind the distance and gerp score variable

# Correlations

```{r}
str(logitdata)

#correlation functions from stephen turner
# Correlation matrix with p-values
cor.prob <- function (X, dfr = nrow(X) - 2) {
  R <- cor(X, use="pairwise.complete.obs")
  above <- row(R) < col(R)
  r2 <- R[above]^2
  Fstat <- r2 * dfr/(1 - r2)
  R[above] <- 1 - pf(Fstat, 1, dfr)
  R[row(R) == col(R)] <- NA
  R
}

# Use this to dump the cor.prob output to a 4 column matrix
flattenSquareMatrix <- function(m) {
  if( (class(m) != "matrix") | (nrow(m) != ncol(m))) stop("Must be a square matrix.") 
  if(!identical(rownames(m), colnames(m))) stop("Row and column names must be equal.")
  ut <- upper.tri(m)
  data.frame(i = rownames(m)[row(m)[ut]],
             j = rownames(m)[col(m)[ut]],
             cor=t(m)[ut],
             p=m[ut])
}

corMasterList <- flattenSquareMatrix(cor.prob(logitdata))
```

# shuffle data first since significance variable is not randomized
```{r}
#set.seed(123)
#g <- runif(nrow(logitdata))
#logitdata <- logitdata[order(g),]
#dim(logitdata)
#2766314       6
#prop.table(table(logitdata$y))
#          0           1 
#0.993937781 0.006062219 
```

# Filtering data

# Randomly choose 10000 rows

```{r}
set.seed(333)
logitrand <- logitdata[sample.int(dim(logitdata)[1], 10000),]
dim(logitrand)
prop.table(table(logitrand$y))
#     0      1 
#0.9954 0.0046
```

# Splitting the data

```{r}


inTrainingSet <- sample(length(logitrand$y),length(logitrand$y)*.7)
train <- logitrand[inTrainingSet,]
test <- logitrand[-inTrainingSet,]

prop.table(table(train$y))   #0.995 0.005 
dim(train)  #7000    6


set.seed(222)
inTrainingSet <- createDataPartition(logitdata$y,p=.7,list=FALSE)
train <- logitdata[inTrainingSet,]
test <- logitdata[-inTrainingSet,]

prop.table(table(train$y))   #0.993906281 0.006093719  
dim(train)  #1936420       6


```

# Using SMOTE function in DMwR package

```{r}
#supersampling rare event
#the classificantion variable has to be a factor
train$y <- as.factor(train$y)
test$y <- as.factor(test$y)
levels(train_smote$y) <- c("No", "Yes")
levels(test$y) <- c("No", "Yes")
#all categorical variables must be factors
str(train)
set.seed(111)
train_smote <- SMOTE(y ~ ., data=train, perc.under = 200)
table(train_smote$y)
#    0     1 
#47200 35400 
prop.table(table(train_smote$y))
dim(train_smote)  #82600     6
```


# Classic GLM method

```{r}
#glm treats second factor level as event of interest
levels(train_smote$y)
#train_smote$y <- as.numeric(as.character(train_smote$y))

glmModel <- glm(y ~ A + B + left_dist + right_dist + closest_dist, 
                data = train_smote, 
                family = binomial)

pred.glmModel <- predict(glmModel, newdata=test, type="response")

roc.glmModel <- roc(test$y, pred.glmModel)

auc.glmModel <- pROC::auc(roc.glmModel)
#0.6164
```

# Establishing tuning/training parameters

```{r}
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 10,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary)
```



# CART model

```{r}

set.seed(2014)

#train_smote$y <- as.numeric(as.character(train_smote$y))
#train_smote$y <- factor(train_smote$y)
levels(train_smote$y) <- c("No", "Yes")

cartModel <- train(y ~ A + B + left_dist + right_dist + closest_dist, data=train_smote, method = "rpart", metric="ROC", trControl = fitControl, tuneLength=5)

pred.cartModel <- as.vector(predict(cartModel, newdata=test, type="prob")[,"Yes"])


roc.cartModel <- pROC::roc(test$y, pred.cartModel)

auc.cartModel <- pROC::auc(roc.cartModel)
#0.4075
```


# Elastic Net

```{r}
set.seed(2014)

eNetModel <- train(y ~ A + B + left_dist + right_dist + closest_dist, data=train_smote, method = "glmnet", metric="ROC", trControl = fitControl, family="binomial", tuneLength=5)

pred.eNetModel <- as.vector(predict(eNetModel, newdata=test, type="prob")[,"Yes"])

roc.eNetModel <- pROC::roc(test$y, pred.eNetModel)

auc.eNetModel <- pROC::auc(roc.eNetModel)
#0.6588
```


# GBM

```{r}
set.seed(2014)

gbmModel <- train(y ~ A + B + left_dist + right_dist + closest_dist, data=train_smote, method = "gbm", metric="ROC", trControl = fitControl, verbose=FALSE, tuneLength=5)

pred.gbmModel <- as.vector(predict(gbmModel, newdata=test, type="prob")[,"Yes"])

roc.gbmModel <- pROC::roc(test$y, pred.gbmModel)

auc.gbmModel <- pROC::auc(roc.gbmModel)
#0.5251
```

# Random Forest

```{r}
set.seed(2014)

rfModel <- train(y ~ A + B + left_dist + right_dist + closest_dist, data=train_smote, method = "rf", metric="ROC", trControl = fitControl, verbose=FALSE, tuneLength=5)

pred.rfModel <- as.vector(predict(rfModel, newdata=test, type="prob")[,"Yes"])

roc.rfModel <- pROC::roc(test$y, pred.rfModel)

auc.rfModel <- pROC::auc(roc.rfModel)
#0.5965
```

# Plotting model performance

```{r}
test.auc <- data.frame(model=c("glm","gbm","glmnet","cart","rForest"),auc=c(auc.glmModel, auc.gbmModel, auc.eNetModel, auc.cartModel, auc.rfModel))

test.auc <- test.auc[order(test.auc$auc, decreasing=TRUE),]

test.auc$model <- factor(test.auc$model, levels=test.auc$model)

test.auc

p<-ggplot(data=test.auc, aes(x=model, y=auc)) +
  geom_bar(stat="identity", fill="steelblue")+
  theme_minimal()
p
```

