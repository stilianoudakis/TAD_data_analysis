---
title: "Modelling Chromosome 1"
author: "Spiro Stilianoudakis"
date: "May 16, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loading Libraries

```{r }
#library(MultiAssayExperiment)
library(GenomicRanges)
#library(IRanges)
library(caret)
library(data.table)
library(gbm)
library(pROC)
library(plyr)
library(dplyr)
library(DMwR)
library(gridExtra)
library(nortest)

packages = c("MultiAssayExperiment", "GenomicRanges", "IRanges", "RaggedExperiment", "SummarizedExperiment", "caret", "data.table", "survival", "gbm", "pROC", "dplyr", "ggplot2", "e1071", "DMwR", "gridExtra", "randomForest", "rpart", "rpart.plot", "glmnet", "gridExtra", "qqman", "rms", "igraph", "ggraph", "RColorBrewer",
"nortest")


package.check <- lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
        install.packages(x, dependencies = TRUE)
        library(x, character.only = TRUE)
    }
})
```

# Reading in data

```{r}

setwd("C:/Users/Spiro Stilianoudakis/Documents/TAD_data/RData/GM12878")

#chromosome 1 filtered/log transformed/no standardization/forward selection

chr1_gm12878_fwd <- readRDS("C:/Users/Spiro Stilianoudakis/Documents/TAD_data/RData/GM12878/chr1_gm12878_fwd.rds")


```


#set number of bootstrap samples

```{r}

bootsamps = 5

```

#set tuning parameters

```{r}
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 5,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary)

```

#create a matrix of row ids that represent the zero class


```{r}
##the number of rows will match the one class
##the number of columns match the number of bootstrap samples


sampids <- matrix(ncol=bootsamps, 
                  nrow=length(chr1_gm12878_fwd$y[which(chr1_gm12878_fwd$y=="Yes")]))


```

#filling in the sample ids matrix

```{r}
set.seed(123)
for(j in 1:bootsamps){
  sampids[,j] <- sample(which(chr1_gm12878_fwd$y=="No"),
                        length(which(chr1_gm12878_fwd$y=="Yes")),
                        replace = TRUE)
}

```

#function for roc curves

```{r}
simple_roc <- function(labels, scores){
  labels <- labels[order(scores, decreasing=TRUE)]
  data.frame(TPR=cumsum(labels)/sum(labels), FPR=cumsum(!labels)/sum(!labels), labels)
}

```


# Random Forest

```{r include=FALSE}

#set length of list objects that will be filled in with specificities
#and sensitivities and aucs and variable importance
rflst <- list(tpr <- matrix(nrow=ceiling((length(which(chr1_gm12878_fwd$y=="Yes"))*2)*.3), 
                              ncol=bootsamps),
                fpr <- matrix(nrow=ceiling((length(which(chr1_gm12878_fwd$y=="Yes"))*2)*.3), 
                              ncol=bootsamps),
                auc <- numeric(bootsamps),
                varimp <- matrix(nrow=dim(chr1_gm12878_fwd)[2]-1,
                                 ncol=bootsamps))
rownames(rflst[[4]]) <- colnames(chr1_gm12878_fwd)[-1]

#Performing Random Forest

for(i in 1:bootsamps){

  #combining the two classes to create balanced data
  data <- rbind.data.frame(chr1_gm12878_fwd[which(chr1_gm12878_fwd$y=="Yes"),],
                           chr1_gm12878_fwd[sampids[,i],])
  
  #determining the best number of variables randomly sampled as candidates at each split
  set.seed(5430)
  bestmtry <- tuneRF(data[,-1],data$y,
                   improve=.01,trace=0,plot=F) 
  bestmtry <- data.frame(bestmtry)
  bestmtry <- bestmtry[order(bestmtry$OOBError, decreasing = FALSE),]
  #bestmtry$mtry[1]

  #splitting the data
  inTrainingSet <- sample(length(data$y),floor(length(data$y)*.7))
  #inTrainingSet <- createDataPartition(data$y,p=.7,list=FALSE)
  train <- data[inTrainingSet,]
  test <- data[-inTrainingSet,]
  
  #determining best number of trees
  tunegrid <- expand.grid(.mtry=bestmtry$mtry[1])
  modellist <- list()
    for (ntree in c(50,200,500,1000)) {
      set.seed(333)
      fit <- train(y~., data=train, 
                   method="rf", 
                   metric="Accuracy",
                   tuneGrid=tunegrid, 
                   trControl=control, 
                   ntree=ntree)
      key <- toString(ntree)
      modellist[[key]] <- fit
    }
    # compare results
    results <- resamples(modellist)
    #summary(results)
    #dotplot(results)
    results <- summary(results)[3]$statistics$Accuracy
    results <- data.frame(results)
    results <- results[order(results$Mean, decreasing = TRUE),]

  set.seed(1006)
  rfModel <- train(y~., data=train, 
                    method="rf", 
                    metric="ROC", 
                    tuneGrid=tunegrid, 
                    trControl=fitControl, 
                    ntree=as.numeric(rownames(results)[1]))


  pred.rfModel <- as.vector(predict(rfModel, 
                                    newdata=test, 
                                    type="prob")[,"Yes"])
  rflst[[1]][,i] <- simple_roc(ifelse(test$y=="Yes",1,0),pred.rfModel)[,1]
  rflst[[2]][,i] <- simple_roc(ifelse(test$y=="Yes",1,0),pred.rfModel)[,2]
  rflst[[3]][i] <- pROC::auc(pROC::roc(test$y, pred.rfModel))
  rflst[[4]][,i] <- varImp(rfModel)$importance[,1]

}



```

# GBM

```{r}
set.seed(2014)

grid <- expand.grid(n.trees=c(50,200,500,1000),shrinkage=c(0.01,0.05,0.1,0.5),n.minobsinnode = c(3,5,10),interaction.depth=c(1,5,10))

#grid <- expand.grid(interaction.depth = seq(6,16, by = 2),
#                    n.trees = seq(500,1500,by=200),
#                    shrinkage = .01, #c(0.01,0.1),
#                    n.minobsinnode = c(10))


gbmModel <- train(y ~ ., data=train, method = "gbm", metric="ROC", trControl = fitControl, verbose=FALSE, tuneGrid=grid)

gbmModel
ggplot(gbmModel) + theme(legend.position = "top")

pred.gbmModel <- as.vector(predict(gbmModel, newdata=test, type="prob")[,"Yes"])

roc.gbmModel <- pROC::roc(test$y, pred.gbmModel)

auc.gbmModel <- pROC::auc(roc.gbmModel)
#0.7925



varImp(gbmModel)
gbm.data <- data.frame(varImp(gbmModel)[1])
gbm.data2 <- cbind.data.frame(Predictors=rownames(gbm.data),Importance=gbm.data$Overall)
gbm.data2 <- gbm.data2[order(gbm.data2$Importance),]
gbm.data2 <- gbm.data2[42:61,]
gbm.data2 <- gbm.data2[order(gbm.data2$Importance),]
gbm.data2$Predictors <- factor(gbm.data2$Predictors,levels=gbm.data2$Predictors)
(p <- ggplot(gbm.data2) + geom_point(aes(Importance,Predictors)) + ggtitle("Variable Importance Plot for GBM Model"))

ggplot(gbm.data2, aes(x=Predictors, 
                        y=Importance)) +
  xlab("Predictors") +
  ylab("Importance") +
  #ggtitle("Importance Plot for Gradient Boosting Machine") +
  geom_bar(stat="identity", 
           width=.5, 
           position="dodge",
           fill="red") +
  coord_flip()
  #theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

# Elastic Net

```{r}

#creating a grid of values for alpha and lambda that minimizes cross validation error
lambda.grid <- 10^seq(2,-2,length=100)
alpha.grid <- seq(0,1,length=10)

searchgrid <- expand.grid(alpha=alpha.grid, lambda=lambda.grid)

set.seed(2014)

eNetModel <- train(y ~ ., data=train, 
                   method = "glmnet",
                   tuneGrid=searchgrid,
                   metric="ROC", 
                   trControl = fitControl, 
                   family="binomial", 
                   standardize=FALSE)
eNetModel$bestTune

#eNetModel <- eNetModel$finalModel

pred.eNetModel <- as.vector(predict(eNetModel, newdata=test, type="prob")[,"Yes"])

roc.eNetModel <- pROC::roc(test$y, pred.eNetModel)

auc.eNetModel <- pROC::auc(roc.eNetModel)
#0.7876



enetdata <- varImp(eNetModel)$importance
enetdata2 <- data.frame(Feature=rownames(enetdata), Importance=enetdata$Overall)
enetdata2$Feature <- as.character(enetdata2$Feature)
enetdata2 <- enetdata2[order(enetdata2$Importance),]
enetdata2 <- enetdata2[42:61,]
enetdata2$Feature <- factor(enetdata2$Feature, levels=enetdata2$Feature[order(enetdata2$Importance)])

enetp <- ggplot(enetdata2, aes(x=Feature, 
                        y=Importance)) +
  xlab("Predictors") +
  ylab("Importance") +
  #ggtitle("Importance Plot for Gradient Boosting Machine") +
  geom_bar(stat="identity", 
           width=.5, 
           position="dodge",
           fill="green") +
  coord_flip()
enetp
  #theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


